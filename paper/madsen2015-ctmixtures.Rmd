
```{r libraries, echo=FALSE, cache=FALSE} 
library(ggplot2) 
library(xtable) 
library(dplyr)
library(magrittr)
library(mmadsenr)
options(tikzDefaultEngine = "xetex")


data_directory <- "/Users/mark/local-research/diss/experiments"

```

# Introduction

A major use of cultural transmission models in archaeology over the past decade has been inference regarding the type of social learning process (or "mode of transmission") operative within past populations.  Identifying the mode of past cultural transmission is important, for example, to testing evolutionary hypotheses for how cumulative cultural transmission and complex culture evolved in the first place [e.g., @BR1985; @CF1981; @Henrich:1998ek; @Wakano:2007gq].  In more recent archaeological settings, the identification of frequency-biased social learning is often linked to the sociopolitical structure of past societies [e.g., @kohler2004].  In fact, a great deal of the theoretical work on cultural transmission within archaeology during the first decade of the twenty-first century focused upon understanding the statistical "signatures" of different modes of transmission and how one might infer such modes from archaeological data [e.g, @Bentley2003; @bentley2007regular; @bentley2004random; @kohler2004; @Mesoudi2009; @shennan2001ceramic; @steele2010ceramic].  

One of the principal lessons of more recent work is that social learning modes which can be easily distinguished in simple models, become less distinguishable in models which incorporate more complex structure and realism, as work on variable population size and time averaging reveals [@Madsen2012TA; @Porcic2014Exploring-the-E; @Premo:2014jv; @Rorabaugh:2014fl]   At the extreme, models can be "equifinal," leading to the same patterns within data despite describing very different processes [@von1949problems].  In particular, more complex cultural transmission models which introduce one or more sources of heterogeneity within a population, will often display empirical patterns which overlap, despite arising from distinct underlying processes [@Mesoudi2009,p42].  Equifinality between theoretical models is a serious concern whenever we study complex systems, and has been discussed in geomorphology, hydrology, climatology, and within archaeology itself [@Aronica:1998dm; @Beven:2006js; @Bonham:2009bi; @Cicchetti:1996gp; @Culling:1987kx; @Marean:1992hg; @Rogers:2000bq; @Savenije:2001fe].  

When considerable equifinality exists between theoretical models, we cannot treat them as distinguishable in our data, despite the fact that we may be able to fit one of the models to our data with acceptable significance values or errors.  I argue that when we cannot distinguish models **up front** using analytical results or statistical analysis of simulation data, we cannot hope to treat them as competing hypotheses in an empirical study.  Where we can distinguish among a model set, however, we can proceeed with reasonable confidence to employ estimation and model selection techniques to infer which model fits a set of empirical data the best.   

The approach taken here is to characterize the behavior of each candidate transmission model across a large number of archaeologically measurable variables through Monte Carlo or agent-based simulation, and then examine our ability to predict the model which generated each data point using standard machine learning classifier systems, whose performance is known to approach optimal.  This approach provides powerful evidence of equifinality where it exists, even for cultural transmission models for which we do not have analytical expressions.  Furthermore, by choosing appropriate classifier algorithms from among those available, we also gain useful information about which variables are effective at predicting which model generated a given data point.  In the case of failures to assign the correct model to a data point, the pattern of failures can be informative as well.  I demonstrate the approach in the case of biased cultural transmission (in various mixtures), in comparison with the typical null hypothesis of neutral or unbiased copying.   

The results indicate that while neutral and biased transmission models can be distinguished very accurately given measurements from entire populations taken without temporal aggregation, the introduction of sampling and the interaction between sampling and temporal aggregation markedly degrades our ability to distinguish between unbiased and biased transmission.  Furthermore, the degredation is not symmetric.  Our ability to properly classify samples which do arise from a biased transmission process remains strong, but samples arising from an unbiased transmission process are subject to disproportionate identification error.  In short, with sampled, time averaged data, we are extremely likely to conclude that samples represent biased transmission, even when this is not the case.  Reducing this equifinality may be possible given the ability to gather large samples, include variables not measured here, or reduce the duration over which samples are time averaged.  Where such reductions in equifinality are not possible, investigators would be well advised to examine different research questions suitable for coarse-grained data.  


# Materials and Methods

Equifinality among theoretical models can arise from two sources.  First, there is strong overlap in the statistical outcomes of stochastic evolutionary models, given the large amount of variation in natural populations.  There may be combinations of parameter values, for example, where two processes yield outcomes which are strongly overlapping across any of the variables we can measure.  This is one of the reasons why Warren Ewens stopped working on neutrality tests for the infinite-alleles model after his seminal works of the 1970's --- given the kind of data available at the level of alleles (rather than sequences), tests for distinguishing neutrality from selection models had little power [@plutinski2004].  I refer to this type of equifinality as __irreducible__.  Irreducibly equifinal transmission models form an __equivalence class__ of models that we cannot distinguish given our data.  Instead, our inferences from data are to the class of models involved.  In such situations, if our research questions focus upon identifying specific modes of transmission, we may lack the ability to give a convincing answer.  In such situations, focusing upon larger scale analysis or coarser-grained models might be advisable.  

Second, equifinality may occur because of our measurement and analysis procedures.  There is growing evidence, for example, that time averaging affects our ability to distinguish biased from neutral transmission with some statistics, but not others [@Premo:2014jv], and that samples with smaller temporal duration suffer less from time averaging effects than deposits of long duration [@Madsen2012TA; @Porcic2014Exploring-the-E; @Premo:2014jv].  When equifinality arises because of the interaction between analytic methods and theoretical models, it may be __reducible__ given different choices of variables and statistics or changing the resolution of data collection where possible.  It may also be the case that some models may be readily distinguished in experimental situations where we can observe transmission chains [e.g., @kempe2014experimental; @mesoudi2014experimental; @schillinger2014copying] but not in pure observational studies.  

With complex 



In formulating a set of cultural transmission models to apply to a set of archaeological data, we need to understand two things, before attempting to fit models to data.  First, are the models statistically distinguishable, in the space of the variables measured?  To the extent that models are not, equifinality exists.  Second, is there evidence that equifinality is reducible in some manner (e.g., by employing additional predictors or finer-grained measurements)?  

The first question is answered by examining the overlap between the distributions of outcomes each model generates.  If there is no overlap, as in the left panel of Figure \ref{img:separability-example}, then we conclude that equifinality between the models is not an issue.  We can proceed with attempts to fit our data to both models, or hypothesis tests.  As the second and third panels show, however, increasing overlap makes it increasingly difficult to employ the values of predictor 1 and predictor 2 to predict which model produced a given data point.  Equifinality exists, to varying degrees, in both of these examples.  Certainly, given the situation depicted in the third panel of Figure \ref{img:separability-example}, we would be ill-advised to conclude that we could correctly distinguish these two models given data.  We can, however, with some precision, tell whether a data point were generated by the set of models $(\mathcal{M}_1, \mathcal{M}_2)$, as opposed to a hypothetical data point in the lower right corner, which could not be fitted to this model set with any confidence.

In formal terms, the ability to correctly assign data points to the data generating process known to have generated them is measured by the _Bayes error_ of a classifier or prediction procedure.  The Bayes error is zero when we can correctly identify each data point as to its model of origin, and rises as two models overlap in the measurement space.  With sufficient overlap, the Bayes error could approach 0.5, which represents a prediction rule which is no better than chance.  This level of error represents full equifinality between models.\footnote{Predictors can achieve even worse error levels, doing worse than coin-flipping, but such rules do not help us assess equifinality, so much as they are signals that we have chosen a poor statistical method for assessing model fit and overlap.}  




\begin{figure}[ht]
\centering
\includegraphics[scale=0.3]{figure/distributional-overlap.pdf}
\caption{Simple example of model outcomes with separable distributions (A), two models with a limited overlap region (B), and two models whose outcomes are highly overlapping, and would cause equifinality in an analysis (C).}
\label{img:separability-example}
\end{figure}  

However, in almost all cases we cannot calculate the Bayes error rate of a classifier rule, because we lack expressions for the probability distribution of model outcomes over the space of measured variables, and in real data sets we usually also lack good prior information about how frequently we expect each model to be represented in the real world.  In fact, Bayes error is directly calculable only for a very few simple models, such as Gaussian class distributions with identical covariance matrices.  There is a large literature, especially in pattern recognition and language classification, on approximating upper bounds for the Bayes error of a classifier, because it is highly useful to know when you cannot improve a recognition system or classifier any further [@Antos:1999dn; @Dobbin:2009du; @McLachlan:1975eo].  Most such upper bounds are based upon parametric models, and use estimates of a distance metric between the classes being distinguished (typically, the Mahalanobis or Bhattacharyya distance) [@devijver1982pattern].  Such bounds are difficult to justify in situations where we have complex social learning models, whose probability density functions in the space of measured variables are typically unknown and are unlikely to be Gaussian.  Nonparametric bounds are possible, using nearest-neighbor methods [@Loizou:1987bi], but in most cases the values obtained are not very tight and may not be useful.  

Instead, Bayes error rates are mostly approximated empirically, by employing classification methods which are known to be good minimizers of test error.  This is the most practical approach for applications.  The general idea will be to simulate a large number of samples from each cultural transmission model whose equifinality we want to assess, train a predictive classifier model on a portion of those simulated samples until we find the smallest error (using cross-validation), and then use the performance of the classifier on the held-out portion of the simulated samples as an unbiased estimate of the true discrimination error between models.  Very low test error would indicate a clean ability to distinguish between models, and thus no equifinality.  As test error increases, the evidence for potential equifinality between models increases. 

## Simulated Samples of Cultural Transmission Models ##

In this study, I examine potential equifinalities between four cultural transmission models, across a range of parameters.  This both demonstrates how to approach equifinality assessment, and will provide a useful look at whether some transmission models commonly discussed by archaeologists are clearly distinguishable using archaeologically measurable variables.  In contrast to previous work comparing "pure" models of social learning modes (e.g., conformism or novelty seeking), in this study I also examine mixed populations, since it is likely that mixtures of social learning modes in a population will increase the potential for equifinality among models.  

All four models employ the Moran dynamics, where one individual engages in a copying event at each elemental step [@moran1962statistical; @moran1958random; @aoki2011rates].  Innovations are modeled using the "infinite alleles" approximation, where every innovation has not been seen in the population previously.  Simulations were performed using the CTMixtures software package, available as open source software.\footnote{\url{https://github.com/mmadsen/ctmixtures}}

\begin{table}[h]
\begin{tabular}{lc}
\hline
Parameter & Value or Interval \\ 
\hline
Innovation rate (in $\theta$ scaled units)  & $[0.1, 5.0]$   \\
Prob. of conformism & $[0.05, 0.25]$ \\
Prob. of anti-conformism & $[0.05, 0.25]$ \\
Sample fractions & 0.1 and 0.2 \\
Time averaging intervals (units of 100 individuals) & 10, 20, 50, 100 \\
Population size & 100 \\
Number of trait dimensions (loci) & 4 \\
Initial traits per dimension & 10 \\
\hline
\end{tabular}
\label{tab:parameters}
\caption{Parameters for simulation runs across the four models studied.  Intervals are treated as prior distributions, and each simulation run is assigned values derived from a uniform random sample on the interval indicated.  Lists of values are all applied to every simulation run (e.g., there is both a 10\% and a 20\% sample from each simulation run.  Single values are applied to every simulation run, and represent a single point prior.)}
\end{table}

In the first model, individuals copy a random individual, resulting in a standard neutral or unbiased transmission model.  The second model is an equal mixture of conformist and anti-conformist individuals, with a randomly chosen intensity of either conformism or anti-conformism from the interval $[0.05, 0.25]$.  The third model is a mixture with 70% conformists and 30% anti-conformists, again with intensity for each chosen at random from the interval $[0.05, 0.25]$.  The final model is the opposite mixture, with 30% conformists and 70% anti-conformists, with intensity randomly chosen from the same interval.  Random selection of innovation and bias parameters from intervals ensures that a large sample of simulation results will cover the full range of outcomes from each model, and it also allows the block of simulated data to be used as a "reference table" in an approximate Bayesian inference (ABC) algorithm.  If we conclude that some set of models are distinguishable and do not suffer from strong equifinalities, we can either use the fitted classifier rules directly, or an approach such as ABC, to then examine other data points and select the best fitting model.  

Simulated populations are 100 individuals in size, although varied and variable population sizes will be examined in a future study.  Each individual carries 4 different traits at any time, which are treated as separate loci or dimensions.  Copying involves no interaction effects between loci in this study.  The population is seeded with 10 randomly chosen traits at each Loci as the initial condition.  The evolution of each simulated population proceeds for 4 million elemental steps, which is equivalent to about 40,000 copying events on average per individual.  This value was chosen by performing simulations at 1 million time step intervals and verifying that the distribution of a key statistic (the number of traits per Loci) had stabilized.  This occurred in most cases between 2 and 3 million steps, and in all cases between 3 and 4 million, so the latter figure was chosen for creating the table of simulated samples for classification analysis.  Each of the four models is replicated, with random parameter selection, for 25,000 replicates.  At the end of 4 million simulation steps, a suite of variables are measured from each of the 25,000 replicates and stored for analysis.  

## Variable Selection ##

Since equifinality is both a function of the transmission model itself, and the variables we employ to measure model outcomes, I measure a number of variables along with different sample strategies (full population census, 10 and 20% samples of the population) and different durations of temporal aggregation (1000, 2000, 5000, and 10000 time steps).  By doing so, we can understand overlaps which are potentially reducible (by increasing sample size, for example) and irreducible equifinalities, which arise between models regardless our measurement methods.  Thus, I measure the same set of predictors (described below) for a population census with no time averaging, and for 8 combinations of sample size and time averaging duration.  

The variables chosen focus upon richness, diversity, trait survival over time, and the Slatkin neutrality test.  Each has either been employed in the archaeological literature on identifying cultural transmission modes, or is a variant on such measures (e.g., IQV is a normalized version of other diversity measures such as entropy, and Neiman's $T_f$ is related to the Inverse Simpson Index).  

In addition to recording the frequency of traits at each of the 4 loci, the traits at each locus were combined into a cross-tabulation which models the process of archaeological classification.  Each class represents a different combination of traits from the 4 loci, and very roughly simulates observing cultural variation through the lens of a standard paradigmatic classification [@Dunnell1971].  Each measurement (e.g., richness, diversity) was applied to each of the four loci, and to the classes or trait configurations formed by intersecting all the loci.  For the locus-centric measures, each statistic was applied to each locus separately, and the mean, minimum, and maximum of the values obtained for each locus were recorded.  I recorded the order statistics in addition to the mean value, since it is possible that minima and maxima might be a better discriminator between models than averages.   
The full list of variables is given in Table \ref{tab:variables}.  

\begin{table}[ht]
\begin{tabular}{lll}
\hline
Variable                                   & Measured Object & Model Variable \\ 
\hline
Trait Configuration Richness      & Class    &  num\_trait\_configurations      \\
Slatkin Exact         & Class   & configuration\_slatkin       \\
Shannon Entropy  & Class &  config\_entropy \\
IQV Diversity  & Class & config\_iqv \\
Neiman $T\_f$ & Class & config\_neiman\_tf \\
Slatkin Exact (Max of Loci)                & Loci   & slatkin\_locus\_max       \\
Slatkin Exact (Min of Loci)                & Loci    & slatkin\_locus\_min      \\
Slatkin Exact (Mean of Loci)                & Loci  & slatkin\_locus\_mean       \\
Shannon Entropy of Trait Frequencies (Min)  & Loci   & entropy\_locus\_max       \\
Shannon Entropy of Trait Frequencies (Max)  & Loci    & entropy\_locus\_min      \\
Shannon Entropy of Trait Frequencies (Mean) & Loci    & entropy\_locus\_mean      \\
IQV Diversity Index (Min)  & Loci  & iqv\_locus\_max \\
IQV Diversity Index (Max)   & Loci & iqv\_locus\_min \\
IQV Diversity Index (Mean)  & Loci & iqv\_locus\_mean \\
Trait Richness (Min) & Loci & richness\_locus\_max \\ 
Trait Richness (Max) & Loci & richness\_locus\_min \\
Trait Richness (Mean)  & Loci & richness\_locus\_mean \\
Kandler-Shennan Trait Survival (Min) & Loci & kandler\_locus\_max \\
Kandler-Shennan Trait Survival (Max) & Loci & kandler\_locus\_min \\
Kandler-Shennan Trait Survival (Mean) & Loci & kandler\_locus\_mean \\
Neiman $T\_f$ (Min) & Loci & neiman\_tf\_locus\_max \\
Neiman $T\_f$ (Max) & Loci & neiman\_tf\_locus\_min \\
Neiman $T\_f$ (Mean) & Loci & neiman\_tf\_locus\_mean \\
\hline

\end{tabular}
\label{tab:variables}
\caption{Variables measured from each transmission model simulation sample.  The middle column records whether the variable is a measurement across traits in a single locus, and then summarized over loci, or whether it applies to configurations of all loci (classes).  The right column records the variable name used within R statistical models, for examining the relative importance of each variable in classifying observations.}
\end{table}

In a first iteration of the analysis, I tried to include the power-law exponent from a log-log transformation of trait frequency, given work by Bentley [REFS], and Mesoudi and Lycett [-@Mesoudi2009].  However, power law exponents vary widely depending upon the exact algorithm used to calculate them [@Clauset:2007p28665; @Gillespie:2014uq].  More seriously, it is not clear that archaeological use of this variable has been comparable to measurements we can make on archaeological assemblages.  Instead of being measured from the prevalence of traits at each time interval during their simulations, Mesoudi and Lycett [-@Mesoudi2009] use the cumulative number of adoptions of each trait over the entire timespan of the simulation as the "frequency" used to calculate power law exponents\footnote{I confirmed this by inspection of the source code for their simulation model, which was provided by Alex Mesoudi.}  Pending a closer investigation of how this variable has been measured by the researchers employing it, I omitted it from the predictor set in the final version of this study.  

The raw data set for this study thus consist of 100,000 measurements of the 23 variables from Table \ref{tab:variables} using a population census (no sampling) and without time averaging, and 800,000 measurements of the same variables, varying across 10\% and 20\% samples of the population, aggregated over durations of 1000, 2000, 5000, and 10000 time steps.  

## Classifier Selection and Training ##







## Classification Error and Equifinality Assessment ##









 

# Results




```{r load-cm, echo=FALSE, cache=FALSE, message=FALSE} 

cm_file <- load(get_data_path(suffix = "experiment-ctmixtures/equifinality-4/results", filename = "cm-merged-gbm.RData"))

# loads object cm_objects, whose keys are experiment names

```



\begin{table}
    \begin{floatrow}[2]
        \ttabbox{
            \caption{Neutral vs Balanced Biased - Sample Size:  10  Duration:  25}
            \label{tab:model-1}}
            {  
```{r xtab-pc, echo=FALSE, results='asis', cache=FALSE, message=FALSE}
xt <- xtable(cm_objects[["population_census"]][["table"]],type='latex',hline.after=c(-1,0,nrow(x)),align="|c|c|c|")
toLatex(xt, comment=FALSE, floating=FALSE)
```
            }
        
        \ttabbox{
            \caption{Neutral vs Balanced Biased - Sample Size:  10  Duration:  10}
                        \label{tab:model-1}}
            {  
```{r xtab-ta1, echo=FALSE, results='asis', cache=FALSE, message=FALSE}
xt <- xtable(cm_objects[["combined_tassize"]][["table"]],type='latex',hline.after=c(-1,0,nrow(x)),align="|c|c|c|")
toLatex(xt, comment=FALSE, floating=FALSE)
```
            }
    \end{floatrow}
    \vskip 0.5in
    \begin{floatrow}[2]
        \ttabbox{
            \caption{Neutral vs Balanced Biased - Sample Size:  10  Duration:  100}
                        \label{tab:model-1}}
            {  
```{r xtab-ta2, echo=FALSE, results='asis', cache=FALSE, message=FALSE}
xt <- xtable(cm_objects[["combined_tassize"]][["table"]],type='latex',hline.after=c(-1,0,nrow(x)),align="|c|c|c|")
toLatex(xt, comment=FALSE, floating=FALSE)
```
            }
        
        \ttabbox{
            \caption{Neutral vs Balanced Biased - Sample Size:  10  Duration:  50}
                        \label{tab:model-1}}
            {   
```{r xtab-ta3, echo=FALSE, results='asis', cache=FALSE, message=FALSE}
xt <- xtable(cm_objects[["combined_tassize"]][["table"]],type='latex',hline.after=c(-1,0,nrow(x)),align="|c|c|c|")
toLatex(xt, comment=FALSE, floating=FALSE)
```
            }
    \end{floatrow}

    \caption{Confusion matrices for four fitted GBM models}

\end{table}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim dignissim nunc, et finibus urna aliquam eget. Donec enim dolor, aliquam sed iaculis vitae, vestibulum sed justo. Curabitur fringilla, mauris quis ultrices mattis, neque libero volutpat nisi, vel mollis mi magna a felis. Phasellus a orci ut elit sodales tristique ac placerat nisi. Maecenas orci purus, ullamcorper non neque vel, imperdiet sollicitudin ante. Duis dapibus ante sed gravida imperdiet. Aenean dapibus augue nec vehicula rhoncus. Mauris ac fermentum ante, eget volutpat lectus. Nunc a est auctor, suscipit augue vel, vulputate lectus. Sed eu elit ullamcorper, interdum neque ut, varius nisi. Phasellus leo justo, mattis rutrum leo vitae, consequat auctor diam. Vivamus cursus, ligula et euismod iaculis, odio nulla ullamcorper ex, vitae cursus mi lacus at sem. Aenean dictum odio dolor, sit amet gravida sem scelerisque vitae. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi.


# Discussion

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim dignissim nunc, et finibus urna aliquam eget. Donec enim dolor, aliquam sed iaculis vitae, vestibulum sed justo. Curabitur fringilla, mauris quis ultrices mattis, neque libero volutpat nisi, vel mollis mi magna a felis. Phasellus a orci ut elit sodales tristique ac placerat nisi. Maecenas orci purus, ullamcorper non neque vel, imperdiet sollicitudin ante. Duis dapibus ante sed gravida imperdiet. Aenean dapibus augue nec vehicula rhoncus. Mauris ac fermentum ante, eget volutpat lectus. Nunc a est auctor, suscipit augue vel, vulputate lectus. Sed eu elit ullamcorper, interdum neque ut, varius nisi. Phasellus leo justo, mattis rutrum leo vitae, consequat auctor diam. Vivamus cursus, ligula et euismod iaculis, odio nulla ullamcorper ex, vitae cursus mi lacus at sem. Aenean dictum odio dolor, sit amet gravida sem scelerisque vitae. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi.





# Acknowledgements

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dignissim dignissim nunc, et finibus urna aliquam eget. Donec enim dolor, aliquam sed iaculis vitae, vestibulum sed justo. Curabitur fringilla, mauris quis ultrices mattis, neque libero volutpat nisi, vel mollis mi magna a felis. Phasellus a orci ut elit sodales tristique ac placerat nisi. Maecenas orci purus, ullamcorper non neque vel, imperdiet sollicitudin ante. 

